# TEAR

An enchanced text encoder by cross-modal pre-training used for acoustic model:


## To-do list

- [x] Release the inference script for Chinese acoustic models
- [ ] Release the pre-training script for your private data




## TEAR Introduction

Text encoders play an important role in text-to-speech (TTS) by analyzing  text input and converting it into linguistic representations.
In order to generate expressive speech from text, pre-training text encoders on large amounts of data has recently become a solution to generate richer and more effective linguistic representations.
However, existing pre-trained text encoders only use the self-supervised target on the text data, without considering the relationship between text and speech modalities during the pre-training stage.
In this paper, we propose TEAR, a cross-modal pre-trained Text Encoder enhanced by Acoustic Representations for TTS. In addition to conventional text pre-training, TEAR incorporates speech pre-training to extract semantic and prosody-related acoustic representations from speech. Then, TEAR introduces a novel cross-modal pre-training task for the text encoder, termed acoustics-aware joint prediction. This task leverages the acoustic representations generated by the preceding speech pre-training, enabling the linguistic representation to perceive and comprehend prosody during the encoding process.
In our implementation, TEAR was pre-trained on 130 million unlabeled  Chinese and English sentences, as well as 740,000 Chinese text-speech pairs.
The results of the downstream TTS experiments on three expressive TTS datasets indicate that the proposed TEAR can encode more effective and comprehensive linguistic representations compared to the text-only pre-trained encoders, leading to the generation of more natural speech. 

<img src="picture/TEAR.jpg" alt="se" width="1000" />

## Usage of the inference script
```shell

```



## License

This project is licensed under the license found in the LICENSE file in the root directory of this source tree.



## Reference

If you find our work is useful in your research, please cite the following paper:

```bibtex
@article{TEAR,
  title   = {TEAR: A Cross-modal Pre-trained Text Encoder Enhanced by Acoustic Representations for Speech Synthesis},
  author  = {Shiming Wang, Yang Ai,, Liping Chen, Yajun Hu, Zhenhua Ling},
  year = {2024}
}
```

## Contact Information

For issues using TEAR, please submit a GitHub issue.

For other communications related to TEAR, please contact `wsmzzz@mail.ustc.edu.cn`.